{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "fake = pd.read_csv('fake_real_news_data/Fake.csv')\n",
    "true = pd.read_csv('fake_real_news_data/True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "true['category'] = 1\n",
    "fake['category'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([true,fake]) #Merging the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d85c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5949e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "sns.set(style = \"whitegrid\",font_scale = 1.2)\n",
    "chart = sns.countplot(x = \"subject\", hue = \"category\" , data = df)\n",
    "chart.set_xticklabels(chart.get_xticklabels(),rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ccc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'] + \" \" + df['title']\n",
    "del df['title']\n",
    "del df['subject']\n",
    "del df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['text']=df['text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Text that is not Fake\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Text that is Fake\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Character count in texts ###\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "text_len=df[df['category']==1]['text'].str.len()\n",
    "ax1.hist(text_len,color='red')\n",
    "ax1.set_title('Original text')\n",
    "text_len=df[df['category']==0]['text'].str.len()\n",
    "ax2.hist(text_len,color='green')\n",
    "ax2.set_title('Fake text')\n",
    "fig.suptitle('Characters in texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word count in texts ###\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "text_len=df[df['category']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(text_len,color='red')\n",
    "ax1.set_title('Original text')\n",
    "text_len=df[df['category']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(text_len,color='green')\n",
    "ax2.set_title('Fake text')\n",
    "fig.suptitle('Words in texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average word length in texts ###\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n",
    "word=df[df['category']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n",
    "ax1.set_title('Original text')\n",
    "word=df[df['category']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n",
    "ax2.set_title('Fake text')\n",
    "fig.suptitle('Average word length in each text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42929dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(text):\n",
    "    words = []\n",
    "    for i in text:\n",
    "        for j in i.split():\n",
    "            words.append(j.strip())\n",
    "    return words\n",
    "corpus = get_corpus(df.text)\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Counting most common words ###\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(corpus)\n",
    "most_common = counter.most_common(10)\n",
    "most_common = dict(most_common)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77279951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_text_ngrams(corpus, n, g):\n",
    "    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unigram analysis ###\n",
    "\n",
    "## I am using \n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "most_common_uni = get_top_text_ngrams(df.text,10,1)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "sns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### bigram analysis ###\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "most_common_bi = get_top_text_ngrams(df.text,10,2)\n",
    "most_common_bi = dict(most_common_bi)\n",
    "sns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### trigram analysis ####\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "most_common_tri = get_top_text_ngrams(df.text,10,3)\n",
    "most_common_tri = dict(most_common_tri)\n",
    "sns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building model ###\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(df.text,df.category,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b51af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b828a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for sequence classification\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528f52c",
   "metadata": {},
   "source": [
    "Refer to the following article for guide with GloVe method.\n",
    "\n",
    "Reference article: https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc000e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
